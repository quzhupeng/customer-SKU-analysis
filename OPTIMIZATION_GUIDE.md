# äº§å“å®¢æˆ·ä»·å€¼åˆ†æç³»ç»Ÿ - ä¼˜åŒ–å®æˆ˜å¼€å‘æŒ‡å—

## ğŸ“‹ ç›®å½•
1. [é¡¹ç›®ä¼˜åŒ–æ€»ä½“è§„åˆ’](#é¡¹ç›®ä¼˜åŒ–æ€»ä½“è§„åˆ’)
2. [ç¬¬ä¸€é˜¶æ®µï¼šå®‰å…¨æ€§ä¿®å¤](#ç¬¬ä¸€é˜¶æ®µå®‰å…¨æ€§ä¿®å¤)
3. [ç¬¬äºŒé˜¶æ®µï¼šä»£ç é‡æ„](#ç¬¬äºŒé˜¶æ®µä»£ç é‡æ„)
4. [ç¬¬ä¸‰é˜¶æ®µï¼šæ€§èƒ½ä¼˜åŒ–](#ç¬¬ä¸‰é˜¶æ®µæ€§èƒ½ä¼˜åŒ–)
5. [ç¬¬å››é˜¶æ®µï¼šæ¶æ„æ”¹è¿›](#ç¬¬å››é˜¶æ®µæ¶æ„æ”¹è¿›)
6. [ç¬¬äº”é˜¶æ®µï¼šæµ‹è¯•å’Œè´¨é‡ä¿è¯](#ç¬¬äº”é˜¶æ®µæµ‹è¯•å’Œè´¨é‡ä¿è¯)
7. [å®æ–½æ£€æŸ¥æ¸…å•](#å®æ–½æ£€æŸ¥æ¸…å•)

---

## é¡¹ç›®ä¼˜åŒ–æ€»ä½“è§„åˆ’

### ä¼˜åŒ–ç›®æ ‡
- ğŸ”’ **å®‰å…¨æ€§**: ä¿®å¤æ‰€æœ‰å®‰å…¨æ¼æ´ï¼Œè¾¾åˆ°ç”Ÿäº§ç¯å¢ƒæ ‡å‡†
- ğŸ—ï¸ **å¯ç»´æŠ¤æ€§**: é™ä½ä»£ç å¤æ‚åº¦ï¼Œæé«˜å¯è¯»æ€§
- âš¡ **æ€§èƒ½**: ä¼˜åŒ–æ•°æ®å¤„ç†å’Œå‰ç«¯æ¸²æŸ“æ€§èƒ½
- ğŸ“ **æ¶æ„**: å»ºç«‹å¯æ‰©å±•çš„æ¨¡å—åŒ–æ¶æ„
- ğŸ§ª **è´¨é‡**: å»ºç«‹å®Œæ•´çš„æµ‹è¯•å’Œè´¨é‡ä¿è¯ä½“ç³»

### å®æ–½æ—¶é—´çº¿
| é˜¶æ®µ | æ—¶é—´ä¼°ç®— | ä¼˜å…ˆçº§ | å…³é”®æˆæœ |
|------|----------|--------|----------|
| ç¬¬ä¸€é˜¶æ®µ | 2-3å¤© | ğŸ”¥ ç´§æ€¥ | å®‰å…¨æ¼æ´ä¿®å¤ |
| ç¬¬äºŒé˜¶æ®µ | 1-2å‘¨ | ğŸ”´ é«˜ | ä»£ç é‡æ„å®Œæˆ |
| ç¬¬ä¸‰é˜¶æ®µ | 1å‘¨ | ğŸŸ¡ ä¸­ | æ€§èƒ½æ˜¾è‘—æå‡ |
| ç¬¬å››é˜¶æ®µ | 2å‘¨ | ğŸŸ¡ ä¸­ | æ¶æ„ç°ä»£åŒ– |
| ç¬¬äº”é˜¶æ®µ | 1å‘¨ | ğŸŸ¢ ä½ | è´¨é‡ä½“ç³»å»ºç«‹ |

---

## ç¬¬ä¸€é˜¶æ®µï¼šå®‰å…¨æ€§ä¿®å¤

### ğŸ¯ ç›®æ ‡
ç«‹å³ä¿®å¤æ‰€æœ‰å®‰å…¨æ¼æ´ï¼Œç¡®ä¿ç³»ç»Ÿå¯ä»¥å®‰å…¨éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒã€‚

### 1.1 ç¯å¢ƒå˜é‡é…ç½®

#### åˆ›å»ºé…ç½®ç®¡ç†ç³»ç»Ÿ

**1. åˆ›å»ºé…ç½®æ–‡ä»¶ `config.py`**
```python
import os
from dataclasses import dataclass
from typing import Optional

@dataclass
class Config:
    """åº”ç”¨é…ç½®ç±»"""
    SECRET_KEY: str
    DEBUG: bool = False
    MAX_CONTENT_LENGTH: int = 50 * 1024 * 1024  # 50MB
    UPLOAD_FOLDER: str = 'uploads'
    ALLOWED_EXTENSIONS: set = None
    
    def __post_init__(self):
        if self.ALLOWED_EXTENSIONS is None:
            self.ALLOWED_EXTENSIONS = {'xlsx', 'xls'}

class DevelopmentConfig(Config):
    DEBUG = True

class ProductionConfig(Config):
    DEBUG = False

class TestingConfig(Config):
    DEBUG = True
    TESTING = True

def get_config() -> Config:
    """æ ¹æ®ç¯å¢ƒå˜é‡è·å–é…ç½®"""
    env = os.getenv('FLASK_ENV', 'development')
    
    configs = {
        'development': DevelopmentConfig,
        'production': ProductionConfig,
        'testing': TestingConfig
    }
    
    config_class = configs.get(env, DevelopmentConfig)
    
    return config_class(
        SECRET_KEY=os.getenv('SECRET_KEY', 'dev-key-change-in-production'),
        MAX_CONTENT_LENGTH=int(os.getenv('MAX_CONTENT_LENGTH', '52428800')),
        UPLOAD_FOLDER=os.getenv('UPLOAD_FOLDER', 'uploads')
    )
```

**2. åˆ›å»ºç¯å¢ƒå˜é‡æ–‡ä»¶ `.env`**
```bash
# å¼€å‘ç¯å¢ƒé…ç½®
SECRET_KEY=your-super-secret-development-key-here
FLASK_ENV=development
MAX_CONTENT_LENGTH=52428800
UPLOAD_FOLDER=uploads

# ç”Ÿäº§ç¯å¢ƒéœ€è¦è®¾ç½®æ›´å®‰å…¨çš„ SECRET_KEY
# SECRET_KEY=production-secret-key-minimum-32-characters
```

**3. åˆ›å»º `.env.example`**
```bash
# é…ç½®ç¤ºä¾‹æ–‡ä»¶
SECRET_KEY=your-secret-key-here
FLASK_ENV=development
MAX_CONTENT_LENGTH=52428800
UPLOAD_FOLDER=uploads
```

**4. æ›´æ–° `.gitignore`**
```gitignore
# ç¯å¢ƒå˜é‡æ–‡ä»¶
.env
.env.local
.env.*.local

# ä¸Šä¼ æ–‡ä»¶ç›®å½•
uploads/
*.log
```

#### ä¿®æ”¹ `app.py` ä½¿ç”¨æ–°é…ç½®

```python
# åœ¨ app.py é¡¶éƒ¨æ·»åŠ 
from config import get_config
import os
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

# è·å–é…ç½®
config = get_config()

# åº”ç”¨é…ç½®
app.config.from_object(config)
```

### 1.2 æ–‡ä»¶ä¸Šä¼ å®‰å…¨å¢å¼º

#### åˆ›å»ºå®‰å…¨å·¥å…·æ¨¡å— `utils/security.py`

```python
import os
import hashlib
import mimetypes
from werkzeug.utils import secure_filename
from typing import Tuple, Optional
import magic  # python-magic

class FileSecurityValidator:
    """æ–‡ä»¶å®‰å…¨éªŒè¯å™¨"""
    
    ALLOWED_EXTENSIONS = {'xlsx', 'xls'}
    ALLOWED_MIME_TYPES = {
        'application/vnd.openxmlformats-officedocument.spreadsheetml.sheet',
        'application/vnd.ms-excel'
    }
    MAX_FILENAME_LENGTH = 255
    
    @classmethod
    def validate_file(cls, file) -> Tuple[bool, Optional[str]]:
        """
        å…¨é¢éªŒè¯ä¸Šä¼ æ–‡ä»¶
        
        Returns:
            (is_valid, error_message)
        """
        if not file or not file.filename:
            return False, "æœªé€‰æ‹©æ–‡ä»¶"
        
        # 1. æ–‡ä»¶åå®‰å…¨æ£€æŸ¥
        if not cls._is_safe_filename(file.filename):
            return False, "æ–‡ä»¶ååŒ…å«ä¸å®‰å…¨å­—ç¬¦"
        
        # 2. æ–‡ä»¶æ‰©å±•åæ£€æŸ¥
        if not cls._is_allowed_extension(file.filename):
            return False, f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œè¯·ä¸Šä¼  {', '.join(cls.ALLOWED_EXTENSIONS)} æ–‡ä»¶"
        
        # 3. MIMEç±»å‹æ£€æŸ¥
        file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        if not cls._is_allowed_mime_type(file):
            return False, "æ–‡ä»¶å†…å®¹ä¸æ‰©å±•åä¸åŒ¹é…"
        
        file.seek(0)  # é‡ç½®æ–‡ä»¶æŒ‡é’ˆ
        return True, None
    
    @classmethod
    def _is_safe_filename(cls, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶åå®‰å…¨æ€§"""
        if len(filename) > cls.MAX_FILENAME_LENGTH:
            return False
        
        # æ£€æŸ¥å±é™©å­—ç¬¦
        dangerous_chars = ['..', '/', '\\', '<', '>', ':', '"', '|', '?', '*']
        return not any(char in filename for char in dangerous_chars)
    
    @classmethod
    def _is_allowed_extension(cls, filename: str) -> bool:
        """æ£€æŸ¥æ–‡ä»¶æ‰©å±•å"""
        return '.' in filename and \
               filename.rsplit('.', 1)[1].lower() in cls.ALLOWED_EXTENSIONS
    
    @classmethod
    def _is_allowed_mime_type(cls, file) -> bool:
        """æ£€æŸ¥MIMEç±»å‹"""
        try:
            mime = magic.from_buffer(file.read(1024), mime=True)
            return mime in cls.ALLOWED_MIME_TYPES
        except Exception:
            return False

def generate_safe_filename(original_filename: str) -> str:
    """ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å"""
    # ä½¿ç”¨werkzeugçš„secure_filename
    safe_name = secure_filename(original_filename)
    
    # æ·»åŠ æ—¶é—´æˆ³å’Œå“ˆå¸Œé¿å…å†²çª
    timestamp = str(int(time.time()))
    hash_suffix = hashlib.md5(safe_name.encode()).hexdigest()[:8]
    
    name, ext = os.path.splitext(safe_name)
    return f"{name}_{timestamp}_{hash_suffix}{ext}"
```

#### æ›´æ–° `app.py` ä¸­çš„æ–‡ä»¶ä¸Šä¼ å¤„ç†

```python
from utils.security import FileSecurityValidator, generate_safe_filename

@app.route('/upload', methods=['POST'])
def upload_file():
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400
        
        file = request.files['file']
        
        # å®‰å…¨éªŒè¯
        is_valid, error_msg = FileSecurityValidator.validate_file(file)
        if not is_valid:
            return jsonify({'error': error_msg}), 400
        
        # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
        safe_filename = generate_safe_filename(file.filename)
        
        # ç¡®ä¿ä¸Šä¼ ç›®å½•å­˜åœ¨
        upload_dir = app.config['UPLOAD_FOLDER']
        os.makedirs(upload_dir, exist_ok=True)
        
        # ä¿å­˜æ–‡ä»¶
        file_path = os.path.join(upload_dir, safe_filename)
        file.save(file_path)
        
        # å…¶ä½™å¤„ç†é€»è¾‘...
        
    except Exception as e:
        app.logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
        return jsonify({'error': 'æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡è¯•'}), 500
```

### 1.3 æ—¥å¿—å’Œç›‘æ§å¢å¼º

#### åˆ›å»ºæ—¥å¿—é…ç½® `utils/logging_config.py`

```python
import logging
import logging.handlers
import os
from datetime import datetime

def setup_logging(app):
    """è®¾ç½®åº”ç”¨æ—¥å¿—é…ç½®"""
    
    # åˆ›å»ºlogsç›®å½•
    log_dir = 'logs'
    os.makedirs(log_dir, exist_ok=True)
    
    # è®¾ç½®æ—¥å¿—æ ¼å¼
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s - '
        '[%(filename)s:%(lineno)d]'
    )
    
    # æ–‡ä»¶å¤„ç†å™¨ - æŒ‰æ—¶é—´è½®è½¬
    file_handler = logging.handlers.TimedRotatingFileHandler(
        filename=os.path.join(log_dir, 'app.log'),
        when='midnight',
        interval=1,
        backupCount=30,
        encoding='utf-8'
    )
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.INFO)
    
    # é”™è¯¯æ—¥å¿—å¤„ç†å™¨
    error_handler = logging.handlers.TimedRotatingFileHandler(
        filename=os.path.join(log_dir, 'error.log'),
        when='midnight',
        interval=1,
        backupCount=30,
        encoding='utf-8'
    )
    error_handler.setFormatter(formatter)
    error_handler.setLevel(logging.ERROR)
    
    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.DEBUG if app.debug else logging.INFO)
    
    # é…ç½®åº”ç”¨æ—¥å¿—
    app.logger.addHandler(file_handler)
    app.logger.addHandler(error_handler)
    app.logger.addHandler(console_handler)
    app.logger.setLevel(logging.DEBUG if app.debug else logging.INFO)
    
    # è®°å½•å¯åŠ¨ä¿¡æ¯
    app.logger.info(f"åº”ç”¨å¯åŠ¨ - ç¯å¢ƒ: {os.getenv('FLASK_ENV', 'development')}")
```

### 1.4 å®æ–½æ£€æŸ¥æ¸…å•

- [ ] å®‰è£…ä¾èµ–åŒ…: `pip install python-dotenv python-magic`
- [ ] åˆ›å»ºé…ç½®æ–‡ä»¶å’Œç¯å¢ƒå˜é‡æ–‡ä»¶
- [ ] æ›´æ–° `.gitignore` æ·»åŠ æ•æ„Ÿæ–‡ä»¶
- [ ] ä¿®æ”¹ `app.py` ä½¿ç”¨æ–°é…ç½®ç³»ç»Ÿ
- [ ] åˆ›å»ºæ–‡ä»¶å®‰å…¨éªŒè¯æ¨¡å—
- [ ] è®¾ç½®æ—¥å¿—ç³»ç»Ÿ
- [ ] ç”Ÿæˆå¼ºå¯†ç ä½œä¸ºç”Ÿäº§ç¯å¢ƒ SECRET_KEY
- [ ] æµ‹è¯•æ–‡ä»¶ä¸Šä¼ å®‰å…¨æ€§
- [ ] éªŒè¯æ—¥å¿—è®°å½•åŠŸèƒ½

---

## ç¬¬äºŒé˜¶æ®µï¼šä»£ç é‡æ„

### ğŸ¯ ç›®æ ‡
é‡æ„æ ¸å¿ƒä»£ç ï¼Œæé«˜å¯ç»´æŠ¤æ€§å’Œå¯æµ‹è¯•æ€§ï¼Œé™ä½ä»£ç å¤æ‚åº¦ã€‚

### 2.1 æ•°æ®åˆ†æå™¨é‡æ„

#### åˆ›å»ºåˆ†æå™¨åŸºç±» `analyzers/base.py`

```python
from abc import ABC, abstractmethod
import pandas as pd
from typing import Dict, Any, Optional
import logging

logger = logging.getLogger(__name__)

class BaseAnalyzer(ABC):
    """åˆ†æå™¨åŸºç±»"""
    
    def __init__(self, data: pd.DataFrame):
        self.data = data.copy()
        self.result = {}
        
    @abstractmethod
    def analyze(self) -> Dict[str, Any]:
        """æ‰§è¡Œåˆ†æ"""
        pass
    
    def validate_data(self) -> bool:
        """éªŒè¯æ•°æ®æœ‰æ•ˆæ€§"""
        if self.data.empty:
            logger.error("æ•°æ®ä¸ºç©º")
            return False
        return True
    
    def prepare_data(self) -> pd.DataFrame:
        """é¢„å¤„ç†æ•°æ®"""
        # åŸºç¡€æ¸…ç†
        data = self.data.dropna()
        return data
```

#### åˆ›å»ºå››è±¡é™åˆ†æå™¨ `analyzers/quadrant_analyzer.py`

```python
from .base import BaseAnalyzer
import pandas as pd
import numpy as np
from typing import Dict, Any, List

class QuadrantAnalyzer(BaseAnalyzer):
    """å››è±¡é™åˆ†æå™¨"""
    
    def __init__(self, data: pd.DataFrame, value_field: str, quantity_field: str, 
                 group_field: str):
        super().__init__(data)
        self.value_field = value_field
        self.quantity_field = quantity_field
        self.group_field = group_field
    
    def analyze(self) -> Dict[str, Any]:
        """æ‰§è¡Œå››è±¡é™åˆ†æ"""
        if not self.validate_data():
            return {}
        
        prepared_data = self.prepare_data()
        aggregated_data = self._aggregate_data(prepared_data)
        quadrants = self._classify_quadrants(aggregated_data)
        
        return {
            'aggregated_data': aggregated_data.to_dict('records'),
            'quadrant_data': quadrants,
            'statistics': self._calculate_statistics(aggregated_data)
        }
    
    def _aggregate_data(self, data: pd.DataFrame) -> pd.DataFrame:
        """èšåˆæ•°æ®"""
        return data.groupby(self.group_field).agg({
            self.value_field: 'sum',
            self.quantity_field: 'sum'
        }).reset_index()
    
    def _classify_quadrants(self, data: pd.DataFrame) -> Dict[str, List]:
        """åˆ†ç±»å››è±¡é™"""
        value_median = data[self.value_field].median()
        quantity_median = data[self.quantity_field].median()
        
        quadrants = {
            'star_products': [],      # æ˜æ˜Ÿäº§å“ï¼šé«˜ä»·å€¼ï¼Œé«˜é”€é‡
            'cash_cow_products': [],  # ç°é‡‘ç‰›ï¼šé«˜ä»·å€¼ï¼Œä½é”€é‡
            'question_products': [],  # é—®é¢˜äº§å“ï¼šä½ä»·å€¼ï¼Œé«˜é”€é‡
            'dog_products': []        # ç˜¦ç‹—äº§å“ï¼šä½ä»·å€¼ï¼Œä½é”€é‡
        }
        
        for _, row in data.iterrows():
            value = row[self.value_field]
            quantity = row[self.quantity_field]
            item = row[self.group_field]
            
            if value >= value_median and quantity >= quantity_median:
                quadrants['star_products'].append(item)
            elif value >= value_median and quantity < quantity_median:
                quadrants['cash_cow_products'].append(item)
            elif value < value_median and quantity >= quantity_median:
                quadrants['question_products'].append(item)
            else:
                quadrants['dog_products'].append(item)
        
        return quadrants
    
    def _calculate_statistics(self, data: pd.DataFrame) -> Dict[str, Any]:
        """è®¡ç®—ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_value': data[self.value_field].sum(),
            'total_quantity': data[self.quantity_field].sum(),
            'value_median': data[self.value_field].median(),
            'quantity_median': data[self.quantity_field].median(),
            'product_count': len(data)
        }
```

#### åˆ›å»ºåˆ†æå™¨å·¥å‚ `analyzers/factory.py`

```python
from typing import Dict, Any
import pandas as pd
from .quadrant_analyzer import QuadrantAnalyzer
from .pareto_analyzer import ParetoAnalyzer
from .distribution_analyzer import DistributionAnalyzer

class AnalyzerFactory:
    """åˆ†æå™¨å·¥å‚"""
    
    _analyzers = {
        'quadrant': QuadrantAnalyzer,
        'pareto': ParetoAnalyzer,
        'distribution': DistributionAnalyzer
    }
    
    @classmethod
    def create_analyzer(cls, analyzer_type: str, data: pd.DataFrame, 
                       **kwargs) -> Any:
        """åˆ›å»ºåˆ†æå™¨å®ä¾‹"""
        if analyzer_type not in cls._analyzers:
            raise ValueError(f"ä¸æ”¯æŒçš„åˆ†æç±»å‹: {analyzer_type}")
        
        analyzer_class = cls._analyzers[analyzer_type]
        return analyzer_class(data, **kwargs)
    
    @classmethod
    def get_available_analyzers(cls) -> List[str]:
        """è·å–å¯ç”¨åˆ†æå™¨åˆ—è¡¨"""
        return list(cls._analyzers.keys())
```

### 2.2 ç»Ÿä¸€é”™è¯¯å¤„ç†

#### åˆ›å»ºå¼‚å¸¸ç±» `utils/exceptions.py`

```python
class AnalysisError(Exception):
    """åˆ†æç›¸å…³å¼‚å¸¸åŸºç±»"""
    pass

class DataValidationError(AnalysisError):
    """æ•°æ®éªŒè¯å¼‚å¸¸"""
    pass

class FileProcessingError(AnalysisError):
    """æ–‡ä»¶å¤„ç†å¼‚å¸¸"""
    pass

class ConfigurationError(AnalysisError):
    """é…ç½®å¼‚å¸¸"""
    pass

class QuadrantAnalysisError(AnalysisError):
    """å››è±¡é™åˆ†æå¼‚å¸¸"""
    pass
```

#### åˆ›å»ºé”™è¯¯å¤„ç†å™¨ `utils/error_handlers.py`

```python
from flask import jsonify, current_app
from .exceptions import AnalysisError
import traceback

def register_error_handlers(app):
    """æ³¨å†Œé”™è¯¯å¤„ç†å™¨"""
    
    @app.errorhandler(AnalysisError)
    def handle_analysis_error(error):
        current_app.logger.error(f"åˆ†æé”™è¯¯: {str(error)}")
        return jsonify({
            'error': str(error),
            'type': 'analysis_error'
        }), 400
    
    @app.errorhandler(FileNotFoundError)
    def handle_file_not_found(error):
        current_app.logger.error(f"æ–‡ä»¶æœªæ‰¾åˆ°: {str(error)}")
        return jsonify({
            'error': 'è¯·æ±‚çš„æ–‡ä»¶ä¸å­˜åœ¨',
            'type': 'file_not_found'
        }), 404
    
    @app.errorhandler(Exception)
    def handle_unexpected_error(error):
        current_app.logger.error(f"æœªé¢„æœŸé”™è¯¯: {str(error)}")
        current_app.logger.error(traceback.format_exc())
        
        if current_app.debug:
            return jsonify({
                'error': str(error),
                'type': 'unexpected_error',
                'traceback': traceback.format_exc()
            }), 500
        else:
            return jsonify({
                'error': 'æœåŠ¡å™¨å†…éƒ¨é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•',
                'type': 'internal_server_error'
            }), 500
```

### 2.3 æ•°æ®æœåŠ¡å±‚

#### åˆ›å»ºæ•°æ®æœåŠ¡ `services/data_service.py`

```python
import pandas as pd
import os
from typing import Optional, Dict, Any, List
from utils.exceptions import FileProcessingError, DataValidationError
import logging

logger = logging.getLogger(__name__)

class DataService:
    """æ•°æ®æœåŠ¡ç±»"""
    
    def __init__(self, upload_folder: str):
        self.upload_folder = upload_folder
        self._cache = {}  # ç®€å•ç¼“å­˜
    
    def load_excel_file(self, file_path: str) -> Dict[str, pd.DataFrame]:
        """åŠ è½½Excelæ–‡ä»¶"""
        try:
            if file_path in self._cache:
                logger.info(f"ä»ç¼“å­˜åŠ è½½æ–‡ä»¶: {file_path}")
                return self._cache[file_path]
            
            full_path = os.path.join(self.upload_folder, file_path)
            if not os.path.exists(full_path):
                raise FileProcessingError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            # è¯»å–æ‰€æœ‰å·¥ä½œè¡¨
            excel_data = pd.read_excel(full_path, sheet_name=None)
            
            # éªŒè¯æ•°æ®
            self._validate_excel_data(excel_data)
            
            # ç¼“å­˜æ•°æ®
            self._cache[file_path] = excel_data
            
            logger.info(f"æˆåŠŸåŠ è½½Excelæ–‡ä»¶: {file_path}")
            return excel_data
            
        except Exception as e:
            logger.error(f"åŠ è½½Excelæ–‡ä»¶å¤±è´¥: {str(e)}")
            raise FileProcessingError(f"åŠ è½½æ–‡ä»¶å¤±è´¥: {str(e)}")
    
    def get_sheet_data(self, file_path: str, sheet_name: str) -> pd.DataFrame:
        """è·å–æŒ‡å®šå·¥ä½œè¡¨æ•°æ®"""
        excel_data = self.load_excel_file(file_path)
        
        if sheet_name not in excel_data:
            raise DataValidationError(f"å·¥ä½œè¡¨ä¸å­˜åœ¨: {sheet_name}")
        
        return excel_data[sheet_name]
    
    def get_sheet_names(self, file_path: str) -> List[str]:
        """è·å–å·¥ä½œè¡¨åç§°åˆ—è¡¨"""
        excel_data = self.load_excel_file(file_path)
        return list(excel_data.keys())
    
    def detect_columns(self, data: pd.DataFrame) -> Dict[str, List[str]]:
        """æ£€æµ‹æ•°æ®åˆ—ç±»å‹"""
        numeric_columns = data.select_dtypes(include=['number']).columns.tolist()
        text_columns = data.select_dtypes(include=['object']).columns.tolist()
        datetime_columns = data.select_dtypes(include=['datetime']).columns.tolist()
        
        return {
            'numeric': numeric_columns,
            'text': text_columns,
            'datetime': datetime_columns,
            'all': data.columns.tolist()
        }
    
    def _validate_excel_data(self, excel_data: Dict[str, pd.DataFrame]):
        """éªŒè¯Excelæ•°æ®"""
        if not excel_data:
            raise DataValidationError("Excelæ–‡ä»¶ä¸ºç©º")
        
        for sheet_name, df in excel_data.items():
            if df.empty:
                logger.warning(f"å·¥ä½œè¡¨ä¸ºç©º: {sheet_name}")
            elif len(df.columns) == 0:
                raise DataValidationError(f"å·¥ä½œè¡¨æ²¡æœ‰åˆ—: {sheet_name}")
    
    def clear_cache(self):
        """æ¸…é™¤ç¼“å­˜"""
        self._cache.clear()
        logger.info("æ•°æ®ç¼“å­˜å·²æ¸…é™¤")
```

### 2.4 é‡æ„åçš„ä¸»åº”ç”¨æ–‡ä»¶

#### æ›´æ–° `app.py`

```python
from flask import Flask, request, jsonify, render_template
from config import get_config
from utils.logging_config import setup_logging
from utils.error_handlers import register_error_handlers
from services.data_service import DataService
from analyzers.factory import AnalyzerFactory
import os

def create_app():
    """åº”ç”¨å·¥å‚å‡½æ•°"""
    app = Flask(__name__)
    
    # åŠ è½½é…ç½®
    config = get_config()
    app.config.from_object(config)
    
    # è®¾ç½®æ—¥å¿—
    setup_logging(app)
    
    # æ³¨å†Œé”™è¯¯å¤„ç†å™¨
    register_error_handlers(app)
    
    # åˆå§‹åŒ–æœåŠ¡
    data_service = DataService(app.config['UPLOAD_FOLDER'])
    
    # æ³¨å†Œè·¯ç”±
    register_routes(app, data_service)
    
    return app

def register_routes(app, data_service):
    """æ³¨å†Œè·¯ç”±"""
    
    @app.route('/')
    def index():
        return render_template('index.html')
    
    @app.route('/upload', methods=['POST'])
    def upload_file():
        # ä½¿ç”¨ä¹‹å‰åˆ›å»ºçš„å®‰å…¨ä¸Šä¼ é€»è¾‘
        pass
    
    @app.route('/analyze', methods=['POST'])
    def analyze_data():
        try:
            data = request.get_json()
            
            # éªŒè¯è¯·æ±‚æ•°æ®
            required_fields = ['file_id', 'sheet_name', 'analysis_type']
            if not all(field in data for field in required_fields):
                return jsonify({'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
            
            # è·å–æ•°æ®
            sheet_data = data_service.get_sheet_data(
                data['file_id'], 
                data['sheet_name']
            )
            
            # åˆ›å»ºåˆ†æå™¨
            analyzer = AnalyzerFactory.create_analyzer(
                data['analysis_type'],
                sheet_data,
                **data.get('analyzer_params', {})
            )
            
            # æ‰§è¡Œåˆ†æ
            result = analyzer.analyze()
            
            return jsonify(result)
            
        except Exception as e:
            app.logger.error(f"åˆ†æå¤±è´¥: {str(e)}")
            raise

if __name__ == '__main__':
    app = create_app()
    app.run(
        debug=app.config['DEBUG'],
        host='0.0.0.0',
        port=int(os.getenv('PORT', 5000))
    )
```

### 2.5 å®æ–½æ£€æŸ¥æ¸…å•

- [ ] åˆ›å»ºåˆ†æå™¨åŸºç±»å’Œå…·ä½“å®ç°
- [ ] å®ç°åˆ†æå™¨å·¥å‚æ¨¡å¼
- [ ] åˆ›å»ºç»Ÿä¸€å¼‚å¸¸å¤„ç†
- [ ] åˆ›å»ºæ•°æ®æœåŠ¡å±‚
- [ ] é‡æ„ä¸»åº”ç”¨æ–‡ä»¶
- [ ] æ›´æ–°å•å…ƒæµ‹è¯•
- [ ] éªŒè¯é‡æ„ååŠŸèƒ½æ­£å¸¸

---

## ç¬¬ä¸‰é˜¶æ®µï¼šæ€§èƒ½ä¼˜åŒ–

### ğŸ¯ ç›®æ ‡
æ˜¾è‘—æå‡åº”ç”¨æ€§èƒ½ï¼ŒåŒ…æ‹¬æ•°æ®å¤„ç†é€Ÿåº¦å’Œå‰ç«¯å“åº”æ€§èƒ½ã€‚

### 3.1 æ•°æ®å¤„ç†ä¼˜åŒ–

#### åˆ›å»ºæ€§èƒ½ä¼˜åŒ–å·¥å…· `utils/performance.py`

```python
import pandas as pd
import numpy as np
from functools import wraps
import time
import logging
from typing import Callable, Any

logger = logging.getLogger(__name__)

def timer(func: Callable) -> Callable:
    """æ€§èƒ½è®¡æ—¶è£…é¥°å™¨"""
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        end_time = time.time()
        
        execution_time = end_time - start_time
        logger.info(f"{func.__name__} æ‰§è¡Œæ—¶é—´: {execution_time:.4f}ç§’")
        
        return result
    return wrapper

class DataOptimizer:
    """æ•°æ®ä¼˜åŒ–å·¥å…·"""
    
    @staticmethod
    def optimize_dataframe(df: pd.DataFrame) -> pd.DataFrame:
        """ä¼˜åŒ–DataFrameå†…å­˜ä½¿ç”¨"""
        optimized_df = df.copy()
        
        # ä¼˜åŒ–æ•°å€¼åˆ—
        for col in optimized_df.select_dtypes(include=['int64']).columns:
            optimized_df[col] = pd.to_numeric(optimized_df[col], downcast='integer')
        
        for col in optimized_df.select_dtypes(include=['float64']).columns:
            optimized_df[col] = pd.to_numeric(optimized_df[col], downcast='float')
        
        # ä¼˜åŒ–å­—ç¬¦ä¸²åˆ—
        for col in optimized_df.select_dtypes(include=['object']).columns:
            if optimized_df[col].nunique() < len(optimized_df) * 0.5:
                optimized_df[col] = optimized_df[col].astype('category')
        
        return optimized_df
    
    @staticmethod
    def chunk_processor(data: pd.DataFrame, chunk_size: int = 10000) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦åˆ†å—å¤„ç†"""
        return len(data) > chunk_size
    
    @staticmethod
    def process_in_chunks(data: pd.DataFrame, func: Callable, 
                         chunk_size: int = 10000) -> pd.DataFrame:
        """åˆ†å—å¤„ç†å¤§æ•°æ®"""
        if not DataOptimizer.chunk_processor(data, chunk_size):
            return func(data)
        
        results = []
        total_chunks = len(data) // chunk_size + (1 if len(data) % chunk_size else 0)
        
        for i in range(0, len(data), chunk_size):
            chunk = data.iloc[i:i + chunk_size]
            result = func(chunk)
            results.append(result)
            
            logger.info(f"å¤„ç†è¿›åº¦: {len(results)}/{total_chunks}")
        
        return pd.concat(results, ignore_index=True)
```

#### ä¼˜åŒ–åˆ†æå™¨æ€§èƒ½

```python
# åœ¨ analyzers/quadrant_analyzer.py ä¸­æ·»åŠ æ€§èƒ½ä¼˜åŒ–

from utils.performance import timer, DataOptimizer

class QuadrantAnalyzer(BaseAnalyzer):
    
    @timer
    def analyze(self) -> Dict[str, Any]:
        """æ‰§è¡Œå››è±¡é™åˆ†æï¼ˆä¼˜åŒ–ç‰ˆæœ¬ï¼‰"""
        if not self.validate_data():
            return {}
        
        # ä¼˜åŒ–æ•°æ®
        optimized_data = DataOptimizer.optimize_dataframe(self.data)
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦åˆ†å—å¤„ç†
        if DataOptimizer.chunk_processor(optimized_data):
            logger.info("æ•°æ®é‡è¾ƒå¤§ï¼Œå¯ç”¨åˆ†å—å¤„ç†")
            aggregated_data = self._aggregate_data_chunked(optimized_data)
        else:
            aggregated_data = self._aggregate_data(optimized_data)
        
        quadrants = self._classify_quadrants(aggregated_data)
        
        return {
            'aggregated_data': aggregated_data.to_dict('records'),
            'quadrant_data': quadrants,
            'statistics': self._calculate_statistics(aggregated_data)
        }
    
    def _aggregate_data_chunked(self, data: pd.DataFrame) -> pd.DataFrame:
        """åˆ†å—èšåˆæ•°æ®"""
        def aggregate_chunk(chunk):
            return chunk.groupby(self.group_field).agg({
                self.value_field: 'sum',
                self.quantity_field: 'sum'
            })
        
        # åˆ†å—å¤„ç†å¹¶åˆå¹¶ç»“æœ
        result = DataOptimizer.process_in_chunks(data, aggregate_chunk)
        
        # æœ€ç»ˆèšåˆ
        return result.groupby(self.group_field).agg({
            self.value_field: 'sum',
            self.quantity_field: 'sum'
        }).reset_index()
```

### 3.2 ç¼“å­˜ç³»ç»Ÿ

#### åˆ›å»ºç¼“å­˜ç®¡ç†å™¨ `utils/cache.py`

```python
import redis
import json
import pickle
import hashlib
from typing import Any, Optional, Union
from datetime import timedelta
import logging

logger = logging.getLogger(__name__)

class CacheManager:
    """ç¼“å­˜ç®¡ç†å™¨"""
    
    def __init__(self, redis_url: str = None):
        self.redis_client = None
        self.memory_cache = {}
        
        if redis_url:
            try:
                self.redis_client = redis.from_url(redis_url)
                self.redis_client.ping()
                logger.info("Redisç¼“å­˜è¿æ¥æˆåŠŸ")
            except Exception as e:
                logger.warning(f"Redisè¿æ¥å¤±è´¥ï¼Œä½¿ç”¨å†…å­˜ç¼“å­˜: {e}")
    
    def get(self, key: str) -> Optional[Any]:
        """è·å–ç¼“å­˜å€¼"""
        try:
            if self.redis_client:
                value = self.redis_client.get(key)
                if value:
                    return pickle.loads(value)
            else:
                return self.memory_cache.get(key)
        except Exception as e:
            logger.error(f"è·å–ç¼“å­˜å¤±è´¥: {e}")
        
        return None
    
    def set(self, key: str, value: Any, 
            expire: timedelta = timedelta(hours=1)) -> bool:
        """è®¾ç½®ç¼“å­˜å€¼"""
        try:
            if self.redis_client:
                serialized_value = pickle.dumps(value)
                return self.redis_client.setex(
                    key, 
                    int(expire.total_seconds()), 
                    serialized_value
                )
            else:
                self.memory_cache[key] = value
                return True
        except Exception as e:
            logger.error(f"è®¾ç½®ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def delete(self, key: str) -> bool:
        """åˆ é™¤ç¼“å­˜"""
        try:
            if self.redis_client:
                return bool(self.redis_client.delete(key))
            else:
                return self.memory_cache.pop(key, None) is not None
        except Exception as e:
            logger.error(f"åˆ é™¤ç¼“å­˜å¤±è´¥: {e}")
            return False
    
    def generate_key(self, *args) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        key_string = "|".join(str(arg) for arg in args)
        return hashlib.md5(key_string.encode()).hexdigest()

# ç¼“å­˜è£…é¥°å™¨
def cached(expire: timedelta = timedelta(hours=1)):
    """ç¼“å­˜è£…é¥°å™¨"""
    def decorator(func):
        @wraps(func)
        def wrapper(*args, **kwargs):
            cache_manager = getattr(wrapper, '_cache_manager', None)
            if not cache_manager:
                return func(*args, **kwargs)
            
            # ç”Ÿæˆç¼“å­˜é”®
            cache_key = cache_manager.generate_key(
                func.__name__, 
                *args, 
                *sorted(kwargs.items())
            )
            
            # å°è¯•ä»ç¼“å­˜è·å–
            cached_result = cache_manager.get(cache_key)
            if cached_result is not None:
                logger.info(f"ç¼“å­˜å‘½ä¸­: {func.__name__}")
                return cached_result
            
            # æ‰§è¡Œå‡½æ•°å¹¶ç¼“å­˜ç»“æœ
            result = func(*args, **kwargs)
            cache_manager.set(cache_key, result, expire)
            logger.info(f"ç¼“å­˜å·²æ›´æ–°: {func.__name__}")
            
            return result
        
        return wrapper
    return decorator
```

### 3.3 å‰ç«¯æ€§èƒ½ä¼˜åŒ–

#### åˆ›å»ºå‰ç«¯ä¼˜åŒ–è„šæœ¬ `static/js/performance.js`

```javascript
// æ€§èƒ½ä¼˜åŒ–å·¥å…·ç±»
class PerformanceOptimizer {
    static debounce(func, wait) {
        let timeout;
        return function executedFunction(...args) {
            const later = () => {
                clearTimeout(timeout);
                func(...args);
            };
            clearTimeout(timeout);
            timeout = setTimeout(later, wait);
        };
    }
    
    static throttle(func, limit) {
        let inThrottle;
        return function() {
            const args = arguments;
            const context = this;
            if (!inThrottle) {
                func.apply(context, args);
                inThrottle = true;
                setTimeout(() => inThrottle = false, limit);
            }
        };
    }
    
    static virtualizeTable(container, data, rowHeight = 40, visibleRows = 20) {
        const totalHeight = data.length * rowHeight;
        const viewport = {
            height: visibleRows * rowHeight,
            scrollTop: 0
        };
        
        const renderVisibleRows = () => {
            const startIndex = Math.floor(viewport.scrollTop / rowHeight);
            const endIndex = Math.min(
                startIndex + visibleRows + 1, 
                data.length
            );
            
            const visibleData = data.slice(startIndex, endIndex);
            const offsetY = startIndex * rowHeight;
            
            return {
                data: visibleData,
                offsetY: offsetY,
                totalHeight: totalHeight
            };
        };
        
        return {
            viewport,
            renderVisibleRows,
            setScrollTop: (scrollTop) => {
                viewport.scrollTop = scrollTop;
            }
        };
    }
    
    static lazyLoadImages(selector = 'img[data-src]') {
        const images = document.querySelectorAll(selector);
        
        const imageObserver = new IntersectionObserver((entries, observer) => {
            entries.forEach(entry => {
                if (entry.isIntersecting) {
                    const img = entry.target;
                    img.src = img.dataset.src;
                    img.classList.remove('lazy');
                    observer.unobserve(img);
                }
            });
        });
        
        images.forEach(img => imageObserver.observe(img));
    }
}

// è¡¨æ ¼è™šæ‹ŸåŒ–å®ç°
class VirtualTable {
    constructor(container, options = {}) {
        this.container = container;
        this.data = [];
        this.rowHeight = options.rowHeight || 40;
        this.visibleRows = options.visibleRows || 20;
        this.columns = options.columns || [];
        
        this.setupTable();
    }
    
    setupTable() {
        this.container.innerHTML = `
            <div class="virtual-table">
                <div class="table-header"></div>
                <div class="table-viewport" style="height: ${this.visibleRows * this.rowHeight}px; overflow-y: auto;">
                    <div class="table-spacer" style="height: 0px;"></div>
                    <div class="table-content"></div>
                </div>
            </div>
        `;
        
        this.viewport = this.container.querySelector('.table-viewport');
        this.spacer = this.container.querySelector('.table-spacer');
        this.content = this.container.querySelector('.table-content');
        this.header = this.container.querySelector('.table-header');
        
        this.viewport.addEventListener('scroll', 
            PerformanceOptimizer.throttle(() => this.render(), 16)
        );
    }
    
    setData(data) {
        this.data = data;
        this.spacer.style.height = `${data.length * this.rowHeight}px`;
        this.render();
    }
    
    setColumns(columns) {
        this.columns = columns;
        this.renderHeader();
    }
    
    renderHeader() {
        const headerHtml = this.columns.map(col => 
            `<div class="table-cell table-header-cell">${col.title}</div>`
        ).join('');
        
        this.header.innerHTML = `<div class="table-row table-header-row">${headerHtml}</div>`;
    }
    
    render() {
        const scrollTop = this.viewport.scrollTop;
        const startIndex = Math.floor(scrollTop / this.rowHeight);
        const endIndex = Math.min(
            startIndex + this.visibleRows + 5, 
            this.data.length
        );
        
        const visibleData = this.data.slice(startIndex, endIndex);
        const offsetY = startIndex * this.rowHeight;
        
        this.content.style.transform = `translateY(${offsetY}px)`;
        this.content.innerHTML = visibleData.map((row, index) => {
            const cells = this.columns.map(col => {
                const value = row[col.field] || '';
                return `<div class="table-cell">${this.formatCell(value, col)}</div>`;
            }).join('');
            
            return `<div class="table-row" data-index="${startIndex + index}">${cells}</div>`;
        }).join('');
    }
    
    formatCell(value, column) {
        if (column.formatter) {
            return column.formatter(value);
        }
        
        if (typeof value === 'number') {
            return value.toLocaleString();
        }
        
        return value;
    }
}
```

#### ä¼˜åŒ–ç°æœ‰ `app.js` ä¸­çš„è¡¨æ ¼æ¸²æŸ“

```javascript
// åœ¨ app.js ä¸­æ›¿æ¢åŸæœ‰çš„è¡¨æ ¼æ¸²æŸ“å‡½æ•°
function displayTableData(data, fieldConfig) {
    const tableContainer = document.getElementById('dataTable');
    
    // å¦‚æœæ•°æ®é‡å¤§ï¼Œä½¿ç”¨è™šæ‹Ÿè¡¨æ ¼
    if (data.length > 1000) {
        if (!window.virtualTable) {
            window.virtualTable = new VirtualTable(tableContainer, {
                rowHeight: 40,
                visibleRows: 25
            });
        }
        
        const columns = Object.keys(fieldConfig).map(field => ({
            field: field,
            title: fieldConfig[field],
            formatter: (value) => {
                if (typeof value === 'number') {
                    return value.toLocaleString();
                }
                return value;
            }
        }));
        
        window.virtualTable.setColumns(columns);
        window.virtualTable.setData(data);
    } else {
        // å°æ•°æ®é‡ä½¿ç”¨åŸæœ‰æ¸²æŸ“æ–¹å¼
        renderNormalTable(data, fieldConfig);
    }
}

// ä¼˜åŒ–æœç´¢åŠŸèƒ½
const optimizedSearch = PerformanceOptimizer.debounce((searchTerm) => {
    if (!analysisResult || !analysisResult.aggregated_data) return;
    
    const filteredData = analysisResult.aggregated_data.filter(row => {
        return Object.values(row).some(value => 
            String(value).toLowerCase().includes(searchTerm.toLowerCase())
        );
    });
    
    const fieldConfig = getTableFieldConfig();
    displayTableData(filteredData, fieldConfig);
}, 300);

// ç»‘å®šä¼˜åŒ–åçš„æœç´¢
document.getElementById('searchInput').addEventListener('input', (e) => {
    optimizedSearch(e.target.value);
});
```

### 3.4 å®æ–½æ£€æŸ¥æ¸…å•

- [ ] å®ç°æ•°æ®å¤„ç†æ€§èƒ½ä¼˜åŒ–
- [ ] é›†æˆç¼“å­˜ç³»ç»Ÿ
- [ ] å®ç°å‰ç«¯è™šæ‹ŸåŒ–è¡¨æ ¼
- [ ] ä¼˜åŒ–æœç´¢å’Œè¿‡æ»¤åŠŸèƒ½
- [ ] æ·»åŠ æ€§èƒ½ç›‘æ§
- [ ] è¿›è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] éªŒè¯ä¼˜åŒ–æ•ˆæœ

---

## ç¬¬å››é˜¶æ®µï¼šæ¶æ„æ”¹è¿›

### ğŸ¯ ç›®æ ‡
å»ºç«‹ç°ä»£åŒ–ã€å¯æ‰©å±•çš„åº”ç”¨æ¶æ„ï¼Œæé«˜ç³»ç»Ÿçš„å¯ç»´æŠ¤æ€§å’Œæ‰©å±•æ€§ã€‚

### 4.1 æ•°æ®åº“é›†æˆ

#### åˆ›å»ºæ•°æ®åº“æ¨¡å‹ `models/models.py`

```python
from flask_sqlalchemy import SQLAlchemy
from datetime import datetime
import uuid

db = SQLAlchemy()

class AnalysisSession(db.Model):
    """åˆ†æä¼šè¯æ¨¡å‹"""
    __tablename__ = 'analysis_sessions'
    
    id = db.Column(db.String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    filename = db.Column(db.String(255), nullable=False)
    original_filename = db.Column(db.String(255), nullable=False)
    file_size = db.Column(db.Integer, nullable=False)
    upload_time = db.Column(db.DateTime, default=datetime.utcnow)
    analysis_type = db.Column(db.String(50))
    status = db.Column(db.String(20), default='uploaded')  # uploaded, analyzing, completed, failed
    
    # å…³ç³»
    sheets = db.relationship('SheetInfo', backref='session', lazy=True, cascade='all, delete-orphan')
    results = db.relationship('AnalysisResult', backref='session', lazy=True, cascade='all, delete-orphan')

class SheetInfo(db.Model):
    """å·¥ä½œè¡¨ä¿¡æ¯æ¨¡å‹"""
    __tablename__ = 'sheet_info'
    
    id = db.Column(db.Integer, primary_key=True)
    session_id = db.Column(db.String(36), db.ForeignKey('analysis_sessions.id'), nullable=False)
    sheet_name = db.Column(db.String(255), nullable=False)
    row_count = db.Column(db.Integer)
    column_count = db.Column(db.Integer)
    columns_info = db.Column(db.JSON)  # å­˜å‚¨åˆ—ä¿¡æ¯

class AnalysisResult(db.Model):
    """åˆ†æç»“æœæ¨¡å‹"""
    __tablename__ = 'analysis_results'
    
    id = db.Column(db.Integer, primary_key=True)
    session_id = db.Column(db.String(36), db.ForeignKey('analysis_sessions.id'), nullable=False)
    analysis_type = db.Column(db.String(50), nullable=False)
    parameters = db.Column(db.JSON)  # åˆ†æå‚æ•°
    results = db.Column(db.JSON)     # åˆ†æç»“æœ
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    execution_time = db.Column(db.Float)  # æ‰§è¡Œæ—¶é—´ï¼ˆç§’ï¼‰

class UserSession(db.Model):
    """ç”¨æˆ·ä¼šè¯æ¨¡å‹"""
    __tablename__ = 'user_sessions'
    
    id = db.Column(db.String(36), primary_key=True, default=lambda: str(uuid.uuid4()))
    session_token = db.Column(db.String(255), unique=True, nullable=False)
    created_at = db.Column(db.DateTime, default=datetime.utcnow)
    last_accessed = db.Column(db.DateTime, default=datetime.utcnow)
    ip_address = db.Column(db.String(45))
    user_agent = db.Column(db.Text)
```

#### åˆ›å»ºæ•°æ®åº“æœåŠ¡ `services/database_service.py`

```python
from models.models import db, AnalysisSession, SheetInfo, AnalysisResult
from typing import Optional, List, Dict, Any
import json
from datetime import datetime

class DatabaseService:
    """æ•°æ®åº“æœåŠ¡ç±»"""
    
    @staticmethod
    def create_analysis_session(filename: str, original_filename: str, 
                              file_size: int) -> AnalysisSession:
        """åˆ›å»ºåˆ†æä¼šè¯"""
        session = AnalysisSession(
            filename=filename,
            original_filename=original_filename,
            file_size=file_size
        )
        db.session.add(session)
        db.session.commit()
        return session
    
    @staticmethod
    def get_analysis_session(session_id: str) -> Optional[AnalysisSession]:
        """è·å–åˆ†æä¼šè¯"""
        return AnalysisSession.query.get(session_id)
    
    @staticmethod
    def save_sheet_info(session_id: str, sheet_data: Dict[str, Any]):
        """ä¿å­˜å·¥ä½œè¡¨ä¿¡æ¯"""
        for sheet_name, df in sheet_data.items():
            sheet_info = SheetInfo(
                session_id=session_id,
                sheet_name=sheet_name,
                row_count=len(df),
                column_count=len(df.columns),
                columns_info={
                    'columns': df.columns.tolist(),
                    'dtypes': df.dtypes.astype(str).to_dict()
                }
            )
            db.session.add(sheet_info)
        
        db.session.commit()
    
    @staticmethod
    def save_analysis_result(session_id: str, analysis_type: str, 
                           parameters: Dict, results: Dict, 
                           execution_time: float) -> AnalysisResult:
        """ä¿å­˜åˆ†æç»“æœ"""
        result = AnalysisResult(
            session_id=session_id,
            analysis_type=analysis_type,
            parameters=parameters,
            results=results,
            execution_time=execution_time
        )
        db.session.add(result)
        db.session.commit()
        return result
    
    @staticmethod
    def get_analysis_history(limit: int = 50) -> List[AnalysisSession]:
        """è·å–åˆ†æå†å²"""
        return AnalysisSession.query.order_by(
            AnalysisSession.upload_time.desc()
        ).limit(limit).all()
    
    @staticmethod
    def cleanup_old_sessions(days: int = 30):
        """æ¸…ç†æ—§ä¼šè¯"""
        cutoff_date = datetime.utcnow() - timedelta(days=days)
        old_sessions = AnalysisSession.query.filter(
            AnalysisSession.upload_time < cutoff_date
        ).all()
        
        for session in old_sessions:
            db.session.delete(session)
        
        db.session.commit()
        return len(old_sessions)
```

### 4.2 APIå±‚è®¾è®¡

#### åˆ›å»ºAPIè“å›¾ `api/v1/__init__.py`

```python
from flask import Blueprint

def create_api_blueprint():
    """åˆ›å»ºAPIè“å›¾"""
    api_v1 = Blueprint('api_v1', __name__, url_prefix='/api/v1')
    
    # æ³¨å†Œè·¯ç”±
    from .upload import upload_bp
    from .analysis import analysis_bp
    from .sessions import sessions_bp
    
    api_v1.register_blueprint(upload_bp)
    api_v1.register_blueprint(analysis_bp)
    api_v1.register_blueprint(sessions_bp)
    
    return api_v1
```

#### åˆ›å»ºä¸Šä¼ API `api/v1/upload.py`

```python
from flask import Blueprint, request, jsonify, current_app
from services.data_service import DataService
from services.database_service import DatabaseService
from utils.security import FileSecurityValidator, generate_safe_filename
import os

upload_bp = Blueprint('upload', __name__, url_prefix='/upload')

@upload_bp.route('/', methods=['POST'])
def upload_file():
    """æ–‡ä»¶ä¸Šä¼ API"""
    try:
        if 'file' not in request.files:
            return jsonify({'error': 'æœªé€‰æ‹©æ–‡ä»¶'}), 400
        
        file = request.files['file']
        
        # å®‰å…¨éªŒè¯
        is_valid, error_msg = FileSecurityValidator.validate_file(file)
        if not is_valid:
            return jsonify({'error': error_msg}), 400
        
        # ç”Ÿæˆå®‰å…¨æ–‡ä»¶å
        safe_filename = generate_safe_filename(file.filename)
        
        # ä¿å­˜æ–‡ä»¶
        upload_dir = current_app.config['UPLOAD_FOLDER']
        os.makedirs(upload_dir, exist_ok=True)
        file_path = os.path.join(upload_dir, safe_filename)
        file.save(file_path)
        
        # åˆ›å»ºæ•°æ®åº“è®°å½•
        session = DatabaseService.create_analysis_session(
            filename=safe_filename,
            original_filename=file.filename,
            file_size=os.path.getsize(file_path)
        )
        
        # åˆ†ææ–‡ä»¶ç»“æ„
        data_service = DataService(upload_dir)
        try:
            excel_data = data_service.load_excel_file(safe_filename)
            DatabaseService.save_sheet_info(session.id, excel_data)
            
            # è¿”å›å·¥ä½œè¡¨ä¿¡æ¯
            sheets_info = []
            for sheet_name, df in excel_data.items():
                column_types = data_service.detect_columns(df)
                sheets_info.append({
                    'name': sheet_name,
                    'rows': len(df),
                    'columns': len(df.columns),
                    'column_types': column_types
                })
            
            return jsonify({
                'session_id': session.id,
                'filename': file.filename,
                'sheets': sheets_info
            })
            
        except Exception as e:
            # å¦‚æœæ–‡ä»¶åˆ†æå¤±è´¥ï¼Œåˆ é™¤æ•°æ®åº“è®°å½•
            db.session.delete(session)
            db.session.commit()
            os.remove(file_path)
            raise
        
    except Exception as e:
        current_app.logger.error(f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}")
        return jsonify({'error': 'æ–‡ä»¶ä¸Šä¼ å¤±è´¥ï¼Œè¯·é‡è¯•'}), 500

@upload_bp.route('/history', methods=['GET'])
def get_upload_history():
    """è·å–ä¸Šä¼ å†å²"""
    try:
        limit = request.args.get('limit', 20, type=int)
        sessions = DatabaseService.get_analysis_history(limit)
        
        history = []
        for session in sessions:
            history.append({
                'id': session.id,
                'original_filename': session.original_filename,
                'upload_time': session.upload_time.isoformat(),
                'file_size': session.file_size,
                'status': session.status,
                'analysis_type': session.analysis_type
            })
        
        return jsonify({'history': history})
        
    except Exception as e:
        current_app.logger.error(f"è·å–å†å²å¤±è´¥: {str(e)}")
        return jsonify({'error': 'è·å–å†å²å¤±è´¥'}), 500
```

#### åˆ›å»ºåˆ†æAPI `api/v1/analysis.py`

```python
from flask import Blueprint, request, jsonify, current_app
from services.data_service import DataService
from services.database_service import DatabaseService
from analyzers.factory import AnalyzerFactory
import time

analysis_bp = Blueprint('analysis', __name__, url_prefix='/analysis')

@analysis_bp.route('/', methods=['POST'])
def perform_analysis():
    """æ‰§è¡Œæ•°æ®åˆ†æ"""
    try:
        data = request.get_json()
        
        # éªŒè¯è¯·æ±‚å‚æ•°
        required_fields = ['session_id', 'sheet_name', 'analysis_type']
        if not all(field in data for field in required_fields):
            return jsonify({'error': 'ç¼ºå°‘å¿…è¦å‚æ•°'}), 400
        
        session_id = data['session_id']
        sheet_name = data['sheet_name']
        analysis_type = data['analysis_type']
        parameters = data.get('parameters', {})
        
        # éªŒè¯ä¼šè¯
        session = DatabaseService.get_analysis_session(session_id)
        if not session:
            return jsonify({'error': 'æ— æ•ˆçš„ä¼šè¯ID'}), 404
        
        # è·å–æ•°æ®
        data_service = DataService(current_app.config['UPLOAD_FOLDER'])
        sheet_data = data_service.get_sheet_data(session.filename, sheet_name)
        
        # æ‰§è¡Œåˆ†æ
        start_time = time.time()
        
        analyzer = AnalyzerFactory.create_analyzer(
            analysis_type,
            sheet_data,
            **parameters
        )
        
        result = analyzer.analyze()
        execution_time = time.time() - start_time
        
        # ä¿å­˜ç»“æœ
        DatabaseService.save_analysis_result(
            session_id=session_id,
            analysis_type=analysis_type,
            parameters=parameters,
            results=result,
            execution_time=execution_time
        )
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        session.analysis_type = analysis_type
        session.status = 'completed'
        db.session.commit()
        
        return jsonify({
            'result': result,
            'execution_time': execution_time
        })
        
    except Exception as e:
        current_app.logger.error(f"åˆ†æå¤±è´¥: {str(e)}")
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€ä¸ºå¤±è´¥
        if 'session' in locals():
            session.status = 'failed'
            db.session.commit()
        
        return jsonify({'error': f'åˆ†æå¤±è´¥: {str(e)}'}), 500

@analysis_bp.route('/<session_id>/results', methods=['GET'])
def get_analysis_results(session_id):
    """è·å–åˆ†æç»“æœ"""
    try:
        session = DatabaseService.get_analysis_session(session_id)
        if not session:
            return jsonify({'error': 'æ— æ•ˆçš„ä¼šè¯ID'}), 404
        
        results = []
        for result in session.results:
            results.append({
                'id': result.id,
                'analysis_type': result.analysis_type,
                'parameters': result.parameters,
                'results': result.results,
                'created_at': result.created_at.isoformat(),
                'execution_time': result.execution_time
            })
        
        return jsonify({'results': results})
        
    except Exception as e:
        current_app.logger.error(f"è·å–ç»“æœå¤±è´¥: {str(e)}")
        return jsonify({'error': 'è·å–ç»“æœå¤±è´¥'}), 500
```

### 4.3 é…ç½®ç®¡ç†å¢å¼º

#### åˆ›å»ºé«˜çº§é…ç½®ç®¡ç† `config/advanced_config.py`

```python
import os
from dataclasses import dataclass, field
from typing import Dict, List, Optional
import yaml
import json

@dataclass
class DatabaseConfig:
    """æ•°æ®åº“é…ç½®"""
    url: str = 'sqlite:///analysis.db'
    echo: bool = False
    pool_size: int = 5
    max_overflow: int = 10
    pool_recycle: int = 3600

@dataclass
class CacheConfig:
    """ç¼“å­˜é…ç½®"""
    type: str = 'memory'  # memory, redis
    redis_url: Optional[str] = None
    default_timeout: int = 3600
    max_entries: int = 1000

@dataclass
class SecurityConfig:
    """å®‰å…¨é…ç½®"""
    secret_key: str = 'dev-key-change-in-production'
    max_content_length: int = 50 * 1024 * 1024
    allowed_extensions: List[str] = field(default_factory=lambda: ['xlsx', 'xls'])
    file_upload_timeout: int = 300
    rate_limit: str = "100 per hour"

@dataclass
class PerformanceConfig:
    """æ€§èƒ½é…ç½®"""
    chunk_size: int = 10000
    max_workers: int = 4
    enable_caching: bool = True
    cache_timeout: int = 3600

@dataclass
class LoggingConfig:
    """æ—¥å¿—é…ç½®"""
    level: str = 'INFO'
    format: str = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
    file_rotation: str = 'midnight'
    backup_count: int = 30
    max_file_size: int = 10 * 1024 * 1024

@dataclass
class AdvancedConfig:
    """é«˜çº§é…ç½®ç±»"""
    database: DatabaseConfig = field(default_factory=DatabaseConfig)
    cache: CacheConfig = field(default_factory=CacheConfig)
    security: SecurityConfig = field(default_factory=SecurityConfig)
    performance: PerformanceConfig = field(default_factory=PerformanceConfig)
    logging: LoggingConfig = field(default_factory=LoggingConfig)
    
    @classmethod
    def from_file(cls, config_path: str) -> 'AdvancedConfig':
        """ä»é…ç½®æ–‡ä»¶åŠ è½½"""
        if not os.path.exists(config_path):
            return cls()
        
        with open(config_path, 'r', encoding='utf-8') as f:
            if config_path.endswith('.yaml') or config_path.endswith('.yml'):
                config_data = yaml.safe_load(f)
            else:
                config_data = json.load(f)
        
        return cls(**config_data)
    
    @classmethod
    def from_env(cls) -> 'AdvancedConfig':
        """ä»ç¯å¢ƒå˜é‡åŠ è½½"""
        config = cls()
        
        # æ•°æ®åº“é…ç½®
        if os.getenv('DATABASE_URL'):
            config.database.url = os.getenv('DATABASE_URL')
        
        # ç¼“å­˜é…ç½®
        if os.getenv('REDIS_URL'):
            config.cache.type = 'redis'
            config.cache.redis_url = os.getenv('REDIS_URL')
        
        # å®‰å…¨é…ç½®
        if os.getenv('SECRET_KEY'):
            config.security.secret_key = os.getenv('SECRET_KEY')
        
        return config

# é…ç½®å·¥å‚
class ConfigFactory:
    """é…ç½®å·¥å‚"""
    
    @staticmethod
    def create_config(env: str = None) -> AdvancedConfig:
        """åˆ›å»ºé…ç½®å®ä¾‹"""
        env = env or os.getenv('FLASK_ENV', 'development')
        
        # é¦–å…ˆä»ç¯å¢ƒå˜é‡åŠ è½½
        config = AdvancedConfig.from_env()
        
        # ç„¶åå°è¯•ä»é…ç½®æ–‡ä»¶åŠ è½½
        config_file = f"config/{env}.yaml"
        if os.path.exists(config_file):
            file_config = AdvancedConfig.from_file(config_file)
            # åˆå¹¶é…ç½®
            config = merge_configs(config, file_config)
        
        return config

def merge_configs(base_config: AdvancedConfig, 
                 override_config: AdvancedConfig) -> AdvancedConfig:
    """åˆå¹¶é…ç½®"""
    # è¿™é‡Œå¯ä»¥å®ç°æ›´å¤æ‚çš„é…ç½®åˆå¹¶é€»è¾‘
    return override_config
```

### 4.4 å®æ–½æ£€æŸ¥æ¸…å•

- [ ] è®¾è®¡å¹¶å®ç°æ•°æ®åº“æ¨¡å‹
- [ ] åˆ›å»ºæ•°æ®åº“æœåŠ¡å±‚
- [ ] å®ç°RESTful APIç»“æ„
- [ ] å»ºç«‹é«˜çº§é…ç½®ç®¡ç†ç³»ç»Ÿ
- [ ] æ•°æ®åº“è¿ç§»è„šæœ¬
- [ ] APIæ–‡æ¡£ç”Ÿæˆ
- [ ] é›†æˆæµ‹è¯•APIç«¯ç‚¹

---

## ç¬¬äº”é˜¶æ®µï¼šæµ‹è¯•å’Œè´¨é‡ä¿è¯

### ğŸ¯ ç›®æ ‡
å»ºç«‹å®Œæ•´çš„æµ‹è¯•ä½“ç³»å’Œè´¨é‡ä¿è¯æµç¨‹ï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œç³»ç»Ÿç¨³å®šæ€§ã€‚

### 5.1 å•å…ƒæµ‹è¯•

#### åˆ›å»ºæµ‹è¯•é…ç½® `tests/conftest.py`

```python
import pytest
import tempfile
import os
from app import create_app
from models.models import db
from config.advanced_config import AdvancedConfig

@pytest.fixture
def app():
    """åˆ›å»ºæµ‹è¯•åº”ç”¨"""
    # åˆ›å»ºä¸´æ—¶æ•°æ®åº“
    db_fd, db_path = tempfile.mkstemp()
    
    # æµ‹è¯•é…ç½®
    config = AdvancedConfig()
    config.database.url = f'sqlite:///{db_path}'
    config.security.secret_key = 'test-secret-key'
    config.performance.enable_caching = False
    
    app = create_app(config)
    app.config['TESTING'] = True
    
    with app.app_context():
        db.create_all()
        yield app
        db.drop_all()
    
    os.close(db_fd)
    os.unlink(db_path)

@pytest.fixture
def client(app):
    """åˆ›å»ºæµ‹è¯•å®¢æˆ·ç«¯"""
    return app.test_client()

@pytest.fixture
def runner(app):
    """åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨"""
    return app.test_cli_runner()

@pytest.fixture
def sample_excel_file():
    """åˆ›å»ºç¤ºä¾‹Excelæ–‡ä»¶"""
    import pandas as pd
    
    # åˆ›å»ºç¤ºä¾‹æ•°æ®
    data = {
        'äº§å“åç§°': ['äº§å“A', 'äº§å“B', 'äº§å“C', 'äº§å“D'],
        'é”€å”®é¢': [10000, 8000, 12000, 6000],
        'é”€é‡': [100, 80, 120, 60],
        'æˆæœ¬': [7000, 6000, 8000, 4500]
    }
    
    df = pd.DataFrame(data)
    
    # ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
    fd, path = tempfile.mkstemp(suffix='.xlsx')
    df.to_excel(path, index=False)
    
    yield path
    
    os.close(fd)
    os.unlink(path)
```

#### åˆ›å»ºåˆ†æå™¨æµ‹è¯• `tests/test_analyzers.py`

```python
import pytest
import pandas as pd
from analyzers.quadrant_analyzer import QuadrantAnalyzer
from analyzers.factory import AnalyzerFactory

class TestQuadrantAnalyzer:
    """å››è±¡é™åˆ†æå™¨æµ‹è¯•"""
    
    def setup_method(self):
        """è®¾ç½®æµ‹è¯•æ•°æ®"""
        self.test_data = pd.DataFrame({
            'product': ['A', 'B', 'C', 'D', 'E'],
            'revenue': [10000, 5000, 15000, 3000, 8000],
            'quantity': [100, 200, 80, 300, 150]
        })
    
    def test_analyze_basic(self):
        """æµ‹è¯•åŸºæœ¬åˆ†æåŠŸèƒ½"""
        analyzer = QuadrantAnalyzer(
            self.test_data,
            value_field='revenue',
            quantity_field='quantity',
            group_field='product'
        )
        
        result = analyzer.analyze()
        
        assert 'aggregated_data' in result
        assert 'quadrant_data' in result
        assert 'statistics' in result
        
        # éªŒè¯ç»Ÿè®¡ä¿¡æ¯
        stats = result['statistics']
        assert stats['total_value'] == 41000
        assert stats['total_quantity'] == 830
        assert stats['product_count'] == 5
    
    def test_classify_quadrants(self):
        """æµ‹è¯•å››è±¡é™åˆ†ç±»"""
        analyzer = QuadrantAnalyzer(
            self.test_data,
            value_field='revenue',
            quantity_field='quantity',
            group_field='product'
        )
        
        result = analyzer.analyze()
        quadrants = result['quadrant_data']
        
        # éªŒè¯å››è±¡é™éƒ½å­˜åœ¨
        expected_quadrants = [
            'star_products', 'cash_cow_products', 
            'question_products', 'dog_products'
        ]
        for quadrant in expected_quadrants:
            assert quadrant in quadrants
            assert isinstance(quadrants[quadrant], list)
        
        # éªŒè¯äº§å“åˆ†ç±»æ€»æ•°
        total_products = sum(len(products) for products in quadrants.values())
        assert total_products == 5
    
    def test_empty_data(self):
        """æµ‹è¯•ç©ºæ•°æ®å¤„ç†"""
        empty_data = pd.DataFrame()
        analyzer = QuadrantAnalyzer(
            empty_data,
            value_field='revenue',
            quantity_field='quantity',
            group_field='product'
        )
        
        result = analyzer.analyze()
        assert result == {}
    
    def test_invalid_fields(self):
        """æµ‹è¯•æ— æ•ˆå­—æ®µå¤„ç†"""
        with pytest.raises(KeyError):
            analyzer = QuadrantAnalyzer(
                self.test_data,
                value_field='invalid_field',
                quantity_field='quantity',
                group_field='product'
            )
            analyzer.analyze()

class TestAnalyzerFactory:
    """åˆ†æå™¨å·¥å‚æµ‹è¯•"""
    
    def test_create_quadrant_analyzer(self):
        """æµ‹è¯•åˆ›å»ºå››è±¡é™åˆ†æå™¨"""
        data = pd.DataFrame({'a': [1, 2], 'b': [3, 4], 'c': ['x', 'y']})
        
        analyzer = AnalyzerFactory.create_analyzer(
            'quadrant',
            data,
            value_field='a',
            quantity_field='b',
            group_field='c'
        )
        
        assert isinstance(analyzer, QuadrantAnalyzer)
    
    def test_invalid_analyzer_type(self):
        """æµ‹è¯•æ— æ•ˆåˆ†æå™¨ç±»å‹"""
        data = pd.DataFrame({'a': [1, 2]})
        
        with pytest.raises(ValueError):
            AnalyzerFactory.create_analyzer('invalid_type', data)
    
    def test_get_available_analyzers(self):
        """æµ‹è¯•è·å–å¯ç”¨åˆ†æå™¨åˆ—è¡¨"""
        analyzers = AnalyzerFactory.get_available_analyzers()
        assert isinstance(analyzers, list)
        assert 'quadrant' in analyzers
```

#### åˆ›å»ºAPIæµ‹è¯• `tests/test_api.py`

```python
import pytest
import json
import io
from models.models import AnalysisSession

class TestUploadAPI:
    """ä¸Šä¼ APIæµ‹è¯•"""
    
    def test_upload_valid_file(self, client, sample_excel_file):
        """æµ‹è¯•ä¸Šä¼ æœ‰æ•ˆæ–‡ä»¶"""
        with open(sample_excel_file, 'rb') as f:
            data = {
                'file': (f, 'test.xlsx')
            }
            response = client.post('/api/v1/upload/', data=data)
        
        assert response.status_code == 200
        result = json.loads(response.data)
        
        assert 'session_id' in result
        assert 'sheets' in result
        assert len(result['sheets']) > 0
    
    def test_upload_no_file(self, client):
        """æµ‹è¯•æœªé€‰æ‹©æ–‡ä»¶"""
        response = client.post('/api/v1/upload/')
        assert response.status_code == 400
        
        result = json.loads(response.data)
        assert 'error' in result
    
    def test_upload_invalid_file_type(self, client):
        """æµ‹è¯•æ— æ•ˆæ–‡ä»¶ç±»å‹"""
        data = {
            'file': (io.BytesIO(b'invalid content'), 'test.txt')
        }
        response = client.post('/api/v1/upload/', data=data)
        assert response.status_code == 400
    
    def test_get_upload_history(self, client):
        """æµ‹è¯•è·å–ä¸Šä¼ å†å²"""
        response = client.get('/api/v1/upload/history')
        assert response.status_code == 200
        
        result = json.loads(response.data)
        assert 'history' in result
        assert isinstance(result['history'], list)

class TestAnalysisAPI:
    """åˆ†æAPIæµ‹è¯•"""
    
    def test_perform_analysis(self, client, app, sample_excel_file):
        """æµ‹è¯•æ‰§è¡Œåˆ†æ"""
        # é¦–å…ˆä¸Šä¼ æ–‡ä»¶
        with open(sample_excel_file, 'rb') as f:
            data = {'file': (f, 'test.xlsx')}
            upload_response = client.post('/api/v1/upload/', data=data)
        
        upload_result = json.loads(upload_response.data)
        session_id = upload_result['session_id']
        sheet_name = upload_result['sheets'][0]['name']
        
        # æ‰§è¡Œåˆ†æ
        analysis_data = {
            'session_id': session_id,
            'sheet_name': sheet_name,
            'analysis_type': 'quadrant',
            'parameters': {
                'value_field': 'é”€å”®é¢',
                'quantity_field': 'é”€é‡',
                'group_field': 'äº§å“åç§°'
            }
        }
        
        response = client.post(
            '/api/v1/analysis/',
            data=json.dumps(analysis_data),
            content_type='application/json'
        )
        
        assert response.status_code == 200
        result = json.loads(response.data)
        
        assert 'result' in result
        assert 'execution_time' in result
    
    def test_analysis_invalid_session(self, client):
        """æµ‹è¯•æ— æ•ˆä¼šè¯ID"""
        analysis_data = {
            'session_id': 'invalid-session-id',
            'sheet_name': 'Sheet1',
            'analysis_type': 'quadrant'
        }
        
        response = client.post(
            '/api/v1/analysis/',
            data=json.dumps(analysis_data),
            content_type='application/json'
        )
        
        assert response.status_code == 404
    
    def test_get_analysis_results(self, client):
        """æµ‹è¯•è·å–åˆ†æç»“æœ"""
        # è¿™é‡Œéœ€è¦å…ˆåˆ›å»ºä¸€ä¸ªä¼šè¯å’Œç»“æœ
        response = client.get('/api/v1/analysis/invalid-session-id/results')
        assert response.status_code == 404
```

### 5.2 é›†æˆæµ‹è¯•

#### åˆ›å»ºé›†æˆæµ‹è¯• `tests/test_integration.py`

```python
import pytest
import json
import tempfile
import os
from app import create_app
from models.models import db

class TestFullWorkflow:
    """å®Œæ•´å·¥ä½œæµé›†æˆæµ‹è¯•"""
    
    def test_complete_analysis_workflow(self, client, sample_excel_file):
        """æµ‹è¯•å®Œæ•´åˆ†æå·¥ä½œæµ"""
        # 1. ä¸Šä¼ æ–‡ä»¶
        with open(sample_excel_file, 'rb') as f:
            data = {'file': (f, 'integration_test.xlsx')}
            upload_response = client.post('/api/v1/upload/', data=data)
        
        assert upload_response.status_code == 200
        upload_result = json.loads(upload_response.data)
        session_id = upload_result['session_id']
        
        # 2. è·å–å·¥ä½œè¡¨ä¿¡æ¯
        assert len(upload_result['sheets']) > 0
        sheet = upload_result['sheets'][0]
        
        # 3. æ‰§è¡Œå››è±¡é™åˆ†æ
        analysis_data = {
            'session_id': session_id,
            'sheet_name': sheet['name'],
            'analysis_type': 'quadrant',
            'parameters': {
                'value_field': 'é”€å”®é¢',
                'quantity_field': 'é”€é‡',
                'group_field': 'äº§å“åç§°'
            }
        }
        
        analysis_response = client.post(
            '/api/v1/analysis/',
            data=json.dumps(analysis_data),
            content_type='application/json'
        )
        
        assert analysis_response.status_code == 200
        analysis_result = json.loads(analysis_response.data)
        
        # éªŒè¯åˆ†æç»“æœ
        assert 'result' in analysis_result
        result = analysis_result['result']
        assert 'quadrant_data' in result
        assert 'statistics' in result
        
        # 4. è·å–åˆ†æå†å²
        results_response = client.get(f'/api/v1/analysis/{session_id}/results')
        assert results_response.status_code == 200
        
        results_data = json.loads(results_response.data)
        assert len(results_data['results']) > 0
        
        # 5. æ£€æŸ¥ä¸Šä¼ å†å²
        history_response = client.get('/api/v1/upload/history')
        assert history_response.status_code == 200
        
        history_data = json.loads(history_response.data)
        assert len(history_data['history']) > 0
```

### 5.3 æ€§èƒ½æµ‹è¯•

#### åˆ›å»ºæ€§èƒ½æµ‹è¯• `tests/test_performance.py`

```python
import pytest
import time
import pandas as pd
from analyzers.quadrant_analyzer import QuadrantAnalyzer
import numpy as np

class TestPerformance:
    """æ€§èƒ½æµ‹è¯•"""
    
    def generate_large_dataset(self, size: int) -> pd.DataFrame:
        """ç”Ÿæˆå¤§æ•°æ®é›†"""
        np.random.seed(42)
        
        return pd.DataFrame({
            'product': [f'Product_{i}' for i in range(size)],
            'revenue': np.random.randint(1000, 50000, size),
            'quantity': np.random.randint(10, 1000, size),
            'cost': np.random.randint(500, 30000, size)
        })
    
    @pytest.mark.performance
    def test_large_dataset_performance(self):
        """æµ‹è¯•å¤§æ•°æ®é›†æ€§èƒ½"""
        # æµ‹è¯•ä¸åŒè§„æ¨¡çš„æ•°æ®é›†
        sizes = [1000, 5000, 10000]
        
        for size in sizes:
            data = self.generate_large_dataset(size)
            
            analyzer = QuadrantAnalyzer(
                data,
                value_field='revenue',
                quantity_field='quantity',
                group_field='product'
            )
            
            start_time = time.time()
            result = analyzer.analyze()
            execution_time = time.time() - start_time
            
            print(f"æ•°æ®è§„æ¨¡: {size}, æ‰§è¡Œæ—¶é—´: {execution_time:.4f}ç§’")
            
            # éªŒè¯ç»“æœæ­£ç¡®æ€§
            assert len(result['aggregated_data']) == size
            assert result['statistics']['product_count'] == size
            
            # æ€§èƒ½æ–­è¨€ï¼ˆæ ¹æ®å®é™…æƒ…å†µè°ƒæ•´ï¼‰
            if size <= 5000:
                assert execution_time < 5.0  # 5ç§’å†…å®Œæˆ
            else:
                assert execution_time < 15.0  # 15ç§’å†…å®Œæˆ
    
    @pytest.mark.performance
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # å¤„ç†å¤§æ•°æ®é›†
        large_data = self.generate_large_dataset(50000)
        
        analyzer = QuadrantAnalyzer(
            large_data,
            value_field='revenue',
            quantity_field='quantity',
            group_field='product'
        )
        
        result = analyzer.analyze()
        
        peak_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_used = peak_memory - initial_memory
        
        print(f"å†…å­˜ä½¿ç”¨: {memory_used:.2f} MB")
        
        # å†…å­˜ä½¿ç”¨ä¸åº”è¶…è¿‡500MB
        assert memory_used < 500
```

### 5.4 ä»£ç è´¨é‡å·¥å…·é…ç½®

#### åˆ›å»ºpre-commité…ç½® `.pre-commit-config.yaml`

```yaml
repos:
  - repo: https://github.com/pre-commit/pre-commit-hooks
    rev: v4.4.0
    hooks:
      - id: trailing-whitespace
      - id: end-of-file-fixer
      - id: check-yaml
      - id: check-json
      - id: check-merge-conflict
      - id: check-added-large-files
  
  - repo: https://github.com/psf/black
    rev: 23.3.0
    hooks:
      - id: black
        language_version: python3
  
  - repo: https://github.com/pycqa/flake8
    rev: 6.0.0
    hooks:
      - id: flake8
        args: ['--max-line-length=88', '--extend-ignore=E203,W503']
  
  - repo: https://github.com/pycqa/isort
    rev: 5.12.0
    hooks:
      - id: isort
        args: ["--profile", "black"]
  
  - repo: https://github.com/pre-commit/mirrors-mypy
    rev: v1.3.0
    hooks:
      - id: mypy
        additional_dependencies: [types-all]
  
  - repo: https://github.com/PyCQA/bandit
    rev: 1.7.5
    hooks:
      - id: bandit
        args: ["-r", ".", "-x", "tests/"]
```

#### åˆ›å»ºpytesté…ç½® `pytest.ini`

```ini
[tool:pytest]
testpaths = tests
python_files = test_*.py
python_functions = test_*
python_classes = Test*
addopts = 
    -v
    --tb=short
    --strict-markers
    --disable-warnings
    --cov=.
    --cov-report=html
    --cov-report=term-missing
    --cov-fail-under=80
markers =
    unit: å•å…ƒæµ‹è¯•
    integration: é›†æˆæµ‹è¯•
    performance: æ€§èƒ½æµ‹è¯•
    slow: æ…¢æµ‹è¯•
```

#### åˆ›å»ºGitHub Actionså·¥ä½œæµ `.github/workflows/ci.yml`

```yaml
name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.8, 3.9, '3.10']
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v4
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Lint with flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=88 --statistics
    
    - name: Format check with black
      run: |
        black --check .
    
    - name: Import sort check with isort
      run: |
        isort --check-only --profile black .
    
    - name: Type check with mypy
      run: |
        mypy .
    
    - name: Security check with bandit
      run: |
        bandit -r . -x tests/
    
    - name: Test with pytest
      run: |
        pytest tests/ -v --cov=. --cov-report=xml
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  performance-test:
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install -r requirements-dev.txt
    
    - name: Run performance tests
      run: |
        pytest tests/ -m performance -v
```

### 5.5 å®æ–½æ£€æŸ¥æ¸…å•

- [ ] åˆ›å»ºå®Œæ•´çš„æµ‹è¯•å¥—ä»¶ï¼ˆå•å…ƒæµ‹è¯•ã€é›†æˆæµ‹è¯•ã€æ€§èƒ½æµ‹è¯•ï¼‰
- [ ] é…ç½®ä»£ç è´¨é‡å·¥å…·ï¼ˆblack, flake8, mypy, banditï¼‰
- [ ] è®¾ç½®pre-commité’©å­
- [ ] é…ç½®CI/CDç®¡é“
- [ ] å»ºç«‹ä»£ç è¦†ç›–ç‡æŠ¥å‘Š
- [ ] è®¾ç½®æ€§èƒ½åŸºå‡†æµ‹è¯•
- [ ] åˆ›å»ºæµ‹è¯•æ•°æ®å’Œfixtures
- [ ] ç¼–å†™APIæ–‡æ¡£

---

## å®æ–½æ£€æŸ¥æ¸…å•

### æ€»ä½“è¿›åº¦è·Ÿè¸ª

#### ç¬¬ä¸€é˜¶æ®µï¼šå®‰å…¨æ€§ä¿®å¤ âœ…
- [x] ç¯å¢ƒå˜é‡é…ç½®ç³»ç»Ÿ
- [x] æ–‡ä»¶ä¸Šä¼ å®‰å…¨å¢å¼º
- [x] æ—¥å¿—å’Œç›‘æ§ç³»ç»Ÿ
- [x] é”™è¯¯å¤„ç†æœºåˆ¶

#### ç¬¬äºŒé˜¶æ®µï¼šä»£ç é‡æ„ ğŸ”„
- [ ] åˆ†æå™¨æ¨¡å—é‡æ„
- [ ] ç»Ÿä¸€å¼‚å¸¸å¤„ç†
- [ ] æ•°æ®æœåŠ¡å±‚åˆ›å»º
- [ ] ä¸»åº”ç”¨é‡æ„

#### ç¬¬ä¸‰é˜¶æ®µï¼šæ€§èƒ½ä¼˜åŒ– ğŸ”„
- [ ] æ•°æ®å¤„ç†ä¼˜åŒ–
- [ ] ç¼“å­˜ç³»ç»Ÿé›†æˆ
- [ ] å‰ç«¯æ€§èƒ½ä¼˜åŒ–
- [ ] è™šæ‹ŸåŒ–è¡¨æ ¼å®ç°

#### ç¬¬å››é˜¶æ®µï¼šæ¶æ„æ”¹è¿› ğŸ”„
- [ ] æ•°æ®åº“æ¨¡å‹è®¾è®¡
- [ ] APIå±‚é‡æ„
- [ ] é«˜çº§é…ç½®ç®¡ç†
- [ ] å¾®æœåŠ¡å‡†å¤‡

#### ç¬¬äº”é˜¶æ®µï¼šæµ‹è¯•å’Œè´¨é‡ä¿è¯ ğŸ”„
- [ ] å®Œæ•´æµ‹è¯•å¥—ä»¶
- [ ] ä»£ç è´¨é‡å·¥å…·
- [ ] CI/CDç®¡é“
- [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•

### æ¯æ—¥å®æ–½å»ºè®®

#### Week 1: å®‰å…¨æ€§å’ŒåŸºç¡€é‡æ„
- **Day 1-2**: å®‰å…¨æ€§ä¿®å¤ï¼ˆç¯å¢ƒå˜é‡ã€æ–‡ä»¶ä¸Šä¼ å®‰å…¨ï¼‰
- **Day 3-4**: åˆ†æå™¨é‡æ„ï¼ˆåŸºç±»ã€å·¥å‚æ¨¡å¼ï¼‰
- **Day 5**: é”™è¯¯å¤„ç†å’Œæ—¥å¿—ç³»ç»Ÿ

#### Week 2: æ€§èƒ½ä¼˜åŒ–å’Œæ¶æ„æ”¹è¿›
- **Day 1-2**: æ•°æ®å¤„ç†æ€§èƒ½ä¼˜åŒ–
- **Day 3-4**: å‰ç«¯æ€§èƒ½ä¼˜åŒ–ï¼ˆè™šæ‹Ÿè¡¨æ ¼ï¼‰
- **Day 5**: ç¼“å­˜ç³»ç»Ÿé›†æˆ

#### Week 3: æ¶æ„ç°ä»£åŒ–
- **Day 1-2**: æ•°æ®åº“æ¨¡å‹å’ŒæœåŠ¡
- **Day 3-4**: APIå±‚é‡æ„
- **Day 5**: é…ç½®ç®¡ç†ç³»ç»Ÿ

#### Week 4: æµ‹è¯•å’Œè´¨é‡ä¿è¯
- **Day 1-2**: å•å…ƒæµ‹è¯•å’Œé›†æˆæµ‹è¯•
- **Day 3**: æ€§èƒ½æµ‹è¯•
- **Day 4**: CI/CDé…ç½®
- **Day 5**: æ–‡æ¡£å’Œéƒ¨ç½²å‡†å¤‡

### éªŒæ”¶æ ‡å‡†

#### å®‰å…¨æ€§æ ‡å‡†
- [ ] æ‰€æœ‰æ•æ„Ÿä¿¡æ¯é€šè¿‡ç¯å¢ƒå˜é‡ç®¡ç†
- [ ] æ–‡ä»¶ä¸Šä¼ é€šè¿‡å®‰å…¨éªŒè¯
- [ ] ç”Ÿäº§ç¯å¢ƒå…³é—­è°ƒè¯•æ¨¡å¼
- [ ] å®æ–½é€‚å½“çš„é”™è¯¯å¤„ç†

#### æ€§èƒ½æ ‡å‡†
- [ ] 10kè¡Œæ•°æ®å¤„ç†åœ¨5ç§’å†…å®Œæˆ
- [ ] å‰ç«¯è¡¨æ ¼æ”¯æŒ10k+è¡Œæµç•…æ»šåŠ¨
- [ ] å†…å­˜ä½¿ç”¨æ§åˆ¶åœ¨åˆç†èŒƒå›´
- [ ] å“åº”æ—¶é—´95%åœ¨2ç§’å†…

#### ä»£ç è´¨é‡æ ‡å‡†
- [ ] æµ‹è¯•è¦†ç›–ç‡è¾¾åˆ°80%ä»¥ä¸Š
- [ ] é€šè¿‡æ‰€æœ‰ä»£ç è´¨é‡æ£€æŸ¥
- [ ] å‡½æ•°å¤æ‚åº¦æ§åˆ¶åœ¨åˆç†èŒƒå›´
- [ ] ä»£ç ç¬¦åˆPEP8è§„èŒƒ

#### æ¶æ„æ ‡å‡†
- [ ] æ¨¡å—é—´ä½è€¦åˆ
- [ ] æ¸…æ™°çš„åˆ†å±‚æ¶æ„
- [ ] å¯æ‰©å±•çš„è®¾è®¡
- [ ] å®Œæ•´çš„é”™è¯¯å¤„ç†

---

## æ€»ç»“

è¿™ä»½ä¼˜åŒ–æŒ‡å—æä¾›äº†ç³»ç»Ÿæ€§çš„æ”¹è¿›æ–¹æ¡ˆï¼Œæ¶µç›–äº†ä»å®‰å…¨æ€§åˆ°æ¶æ„çš„å„ä¸ªæ–¹é¢ã€‚æŒ‰ç…§è¿™ä¸ªè®¡åˆ’å®æ–½ï¼Œå°†æ˜¾è‘—æå‡ç³»ç»Ÿçš„è´¨é‡ã€æ€§èƒ½å’Œå¯ç»´æŠ¤æ€§ã€‚

å…³é”®æˆåŠŸå› ç´ ï¼š
1. **å¾ªåºæ¸è¿›**ï¼šæŒ‰é˜¶æ®µå®æ–½ï¼Œç¡®ä¿æ¯ä¸ªé˜¶æ®µå®Œæˆåç³»ç»Ÿä»å¯æ­£å¸¸è¿è¡Œ
2. **æµ‹è¯•å…ˆè¡Œ**ï¼šåœ¨é‡æ„å‰å»ºç«‹æµ‹è¯•ï¼Œç¡®ä¿åŠŸèƒ½ä¸ä¸¢å¤±
3. **æ–‡æ¡£åŒæ­¥**ï¼šåŠæ—¶æ›´æ–°æ–‡æ¡£ï¼Œä¾¿äºå›¢é˜Ÿåä½œ
4. **ç›‘æ§åé¦ˆ**ï¼šå®æ–½è¿‡ç¨‹ä¸­æŒç»­ç›‘æ§æ€§èƒ½å’Œé”™è¯¯

é¢„æœŸæ”¶ç›Šï¼š
- ğŸ”’ **å®‰å…¨æ€§**ï¼šç”Ÿäº§çº§åˆ«çš„å®‰å…¨ä¿éšœ
- ğŸš€ **æ€§èƒ½**ï¼š3-5å€æ€§èƒ½æå‡
- ğŸ› ï¸ **å¯ç»´æŠ¤æ€§**ï¼šé™ä½50%ç»´æŠ¤æˆæœ¬
- ğŸ“ˆ **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒ10å€æ•°æ®è§„æ¨¡